{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#welcome-to-brain-pages","title":"Welcome to Brain Pages!","text":"<p>This website is a subset of my second brain (ie. where I store tips and tricks, lessons learnt etc.), in particular, what I struggled to find on the web.</p> <p>Since I took note of it already, I may as well share it... Hopefully that could be useful to someone!</p> <p>Contact: please react using issues on GitHub, or try Poulti on Twitter</p> <p>Note</p> <p>The content of the website is generated automatically from this GitHub repo.</p> <p>If the website is broken, it might actually be updating, try again a few minutes later.</p>"},{"location":"autonomous-underearth-house/","title":"Autonomous underearth house","text":"<p>This is a brainstorming about having an off grid, independent house/bunker, and buried under the earth</p>"},{"location":"autonomous-underearth-house/#challenges-to-address","title":"Challenges to address","text":""},{"location":"autonomous-underearth-house/#location-environment-based","title":"Location / environment based","text":""},{"location":"autonomous-underearth-house/#storm-drains","title":"Storm drains","text":"<ul> <li>Not at the bottom of a hill/next to water (contradicts the water turbine)</li> <li>need to check how the underground layers usually are laid out and how the water flows downhill. Construction may need to be like a \"boat\"</li> </ul>"},{"location":"autonomous-underearth-house/#earthquake","title":"Earthquake","text":"<ul> <li>Not sure the build can do much about this --&gt; avoid locations prone to it. I guess the UK is OK for that.</li> </ul>"},{"location":"autonomous-underearth-house/#stormtornado","title":"Storm/tornado","text":"<p>Shouldn't be too much of a problem under earth.</p>"},{"location":"autonomous-underearth-house/#build-of-the-house","title":"Build of the house","text":""},{"location":"autonomous-underearth-house/#waterproofdamp-proof","title":"Waterproof/damp proof","text":"<ul> <li>maybe a container type, like the garden house (good isolation and watertight)</li> </ul>"},{"location":"autonomous-underearth-house/#ventilation-filtration-system","title":"Ventilation + filtration system","text":"<ul> <li>Heat pump with filters to purify the air in addition to warming or cooling </li> </ul>"},{"location":"autonomous-underearth-house/#primary-input-and-output","title":"Primary input and output","text":""},{"location":"autonomous-underearth-house/#electricity-generation","title":"Electricity generation","text":"<ul> <li>Solar panels, would take a large area</li> <li>Water turbine from river</li> <li>Wind turbine</li> </ul>"},{"location":"autonomous-underearth-house/#water-production","title":"Water production","text":"<ul> <li>Filter river/mountain water nearby?</li> <li>Recycle grey water</li> <li>Filter rain water</li> </ul>"},{"location":"autonomous-underearth-house/#sewage-system","title":"Sewage system","text":"<ul> <li>Must have a difference between tap/shower water and toilets flush. Maybe two different treatments?</li> <li>Grey and foul? In isolation (in the middle of nowhere) could the second one be treated to be released with the first? Like a mixer?</li> <li>Off grid toilets system seem to move waste mechanically (with a manually actioned pump) and turn the waste into fertilizer with bacteria - so you don't need the sewer at all. Some could even generate gas? I don't know if you can do something with it</li> </ul>"},{"location":"autonomous-underearth-house/#secondary","title":"Secondary","text":""},{"location":"autonomous-underearth-house/#heating-cooling","title":"Heating / cooling","text":"<ul> <li>Via the ventilation system - will need an external mini house to have the exhausts/external units</li> </ul>"},{"location":"autonomous-underearth-house/#heating-water","title":"Heating water","text":"<ul> <li>Gas is impossible to source naturally -- actually... maybe it could be recycled from the toilets, but not sure it's a good idea to create a dependency on going to sh... to take a warm shower :D</li> <li>Electricity based?</li> </ul>"},{"location":"autonomous-underearth-house/#access","title":"Access","text":"<ul> <li>How to get inside in a safe way depending on the weather? Like a submarine entrance?</li> <li>In any case, likely planning a small room to clean shoes and stuff</li> </ul>"},{"location":"autonomous-underearth-house/#oxygenco2-levels-emergency-alarm","title":"Oxygen/CO2 levels + emergency alarm?","text":"<p>ie. what if the vent for the Aircon gets stuck and CO2 level raises. What do they do in a submarine?</p>"},{"location":"docker-from-compose-to-swarm/","title":"Docker from compose to swarm","text":"<p>A homelab story from 2023 to 2025+...</p>"},{"location":"docker-from-compose-to-swarm/#how-to-migrate-a-docker-compose-file-to-docker-swarm-mode","title":"How to migrate a docker compose file to Docker Swarm Mode?","text":"<p>...from <code>docker compose up -d</code> to <code>docker stack deploy --compose-file docker-compose.yaml ha</code></p>"},{"location":"docker-from-compose-to-swarm/#0-backstory-home-assistant-in-a-container","title":"0. Backstory: Home Assistant in a container","text":"<p>How did it start... for context:</p> <p>Like a good chunk of 30yo+, I bought a (couple of) Raspberry without knowing what to do with it...</p> <p>After installing a PKI and OpenVPN manually on a Raspberry Pi 4 (and that's a lot of commands), I've been wanting a more flexible way to install/remove/update stuff on it - particularly because I hate having apps installed directly on the OS, and the thought of needing to reinstall and configure everything should I need to replace/upgrade the hardware. Enter containers*: a packaged system/app I can deploy with 1 action/no install, and when it stops, it (almost) doesn't leave anything to clean up.</p> <p>But what if I want to run more things? and being resilient to the raspberry or sd card dying? That'd be fun to have a cluster of Raspberry Pi... which led me to find the Turing Pi 2 cluster board. And as I researched what people usually deploy on Raspberry clusters, I kept seeing the Home Assistant logo...</p> <p>So I looked at it up close while waiting for the Turing Pi 2 to release - I thought I would run the containers directly on my desktop using Docker Compose. And if I did it right, I would \"just\" have to move my containers to the new hardware once ready, right?... well, not that simple unfortunately...</p> <p>This page is the combined notes going through that journey, because for many topics I struggled to find clarity, I thought I'd make this public. I'm not a professional blogger so please excuse the clunky style - just hope it is useful to someone somewhere!</p>"},{"location":"docker-from-compose-to-swarm/#a-first-docker-compose-running-on-windows-docker-desktop","title":"A first docker compose, running on windows / docker desktop","text":"<p>So I started creating a docker compose file to run on Docker for Windows, with just Home Assistant to begin with. That's pretty simple at that stage, you can find examples of docker compose file on the official website. Below is the simplified copy of one of the first compose file:</p> <pre><code>version: '3.9'\n\nservices:\n  homeassistant:\n    container_name: homeassistant\n    hostname: homeassistant\n    image: \"ghcr.io/home-assistant/home-assistant:stable\"\n    restart: always\n    ports:\n      - \"8123:8123/tcp\"\n    environment:\n      - TZ=Europe/London\n    volumes:\n      - hass-config:/config\n      - /etc/localtime:/etc/localtime:ro\n\nvolumes:\n  hass-config:\n    driver: local\n    driver_opts:\n      type: none\n      device: D:\\HomeAssistant\\hass\\config\n      o: bind\n</code></pre> <p>Here are the main things to know about the sections of a docker compose file:</p> <ul> <li> <p><code>services</code>: this where you declare the \"machine\" (container) and detail settings </p> </li> <li> <p><code>volumes</code>: this is the configuration of storage volumes for the container. The point is to store your custom config for the machine outside the container, to be able to back it up and restart the container in the same state.</p> </li> <li>In this Windows hosted container, I configured the storage to map (bind) the volume named <code>hass-config</code> to a local folder on my Windows machine, at <code>D:\\HomeAssistant\\hass\\config</code></li> </ul> <p>...but the point is to automate and control \"things\" remotely, so I added a Zigbee controller, buying a \u00a315 Zonoff USB Zigbee dongle on Amazon. To make it work, decided to use zigbee2mqtt as the service to connect with it, and hence I needed an MQTT server (eg mosquitto) in between home assistant and zigbee2mqtt. MQTT is a communication protocol commonly used in industrial environment. Here, it's used to link the ZigBee receiver, and Home Assistant.</p> <p>Updated docker compose looked like this:</p> <pre><code>version: '3.9'\n\nservices:\n  homeassistant:\n    container_name: homeassistant\n    hostname: homeassistant\n    image: \"ghcr.io/home-assistant/home-assistant:stable\"\n    restart: always\n    ports:\n      - \"8123:8123/tcp\"\n    environment:\n      - TZ=Europe/London\n    volumes:\n      - hass-config:/config\n      - /etc/localtime:/etc/localtime:ro\n\n  mosquitto:\n    container_name: mosquitto\n    hostname: mosquitto\n    image: eclipse-mosquitto\n    restart: always\n    ports:\n      - 1883:1883\n      - 9001:9001\n    environment:\n      - TZ=Europe/London\n    volumes:\n      - mosquitto-config:/mosquitto:rw\n\n  zigbee2mqtt:\n    container_name: zigbee2mqtt\n    hostname: zigbee2mqtt\n    image: koenkk/zigbee2mqtt\n    restart: always\n    ports:\n      - 8080:8080\n    environment:\n      - TZ=Europe/London\n    volumes:\n      - zigbee2mqtt-data:/app/data\n    devices:\n      - /dev/ttyUSB0:/dev/zigbee\n\n\nvolumes:\n  hass-config:\n    driver: local\n    driver_opts:\n      type: none\n      device: D:\\HomeAssistant\\hass\\config\n      o: bind\n  mosquitto-config:\n    driver: local\n    driver_opts:\n      type: none\n      device: D:\\HomeAssistant\\mosquitto\n      o: bind\n  zigbee2mqtt-data:\n    driver: local\n    driver_opts:\n      type: none\n      device: D:\\HomeAssistant\\zigbee2mqtt-data\n      o: bind\n</code></pre> <p>Notice the addition of the USB device used by Zigbee2MQTT container? </p><pre><code>  zigbee2mqtt:\n  (...)\n    devices:\n      - /dev/ttyUSB0:/dev/zigbee\n</code></pre><p></p> <p>As the containers are actually run in Windows Subsystem for Linux (aka WSL, a linux environment in Windows OS) the <code>devices</code> reference above needs to give the path to a USB device known to the WSL... so how do you get a USB device connected to your Windows machine, to the WSL?</p> <p>Enters USBIP-WIN: \"Windows software for sharing locally connected USB devices to other machines, including Hyper-V guests and WSL 2.\" according to its github.</p> <p>Wait, it says \"WSL 2\"? By default, Windows uses WSL \"1\", so you'll find below how to switch to WSL 2 and install a linux distribution in it. Here are some of notes on the process:</p> <ul> <li>In BIOS, turn on VT-x</li> <li>In a cmd or powershell run as admin:   <pre><code>wsl --install\n</code></pre></li> <li>Reboot</li> <li>Update the WSL   <pre><code>wsl --update\n</code></pre></li> <li>Set it to WSL 2   <pre><code>wsl --set-default-version 2\n</code></pre></li> <li>Download a distro, eg Ubuntu 22.04 LTS, go to https://aka.ms/wslubuntu2204 (Go to Microsoft Store app, and search Ubuntu, download), and open the .appbundle</li> <li>Install the distro (not sure that step is necessary anymore if you opened the appbundle in the UI):    <pre><code>wsl --install -d ubuntu\n</code></pre></li> </ul> <p>Then follow the steps to install USBIP-WIN from the github above.</p> <p>At that stage you have the foundation of home assistant and docker services, and ZigBee network connected. Time to expand to even more services!</p>"},{"location":"docker-from-compose-to-swarm/#adding-more-container-to-compose-and-more-complexity","title":"Adding more container to compose... and more complexity","text":"<p>From here, everything is possible - without a specific order, you can add a variety of things:</p> <ul> <li>An RFlink device (USB) to access weather stations signals (radio frequencies on 433MHz)</li> <li>A back up service for the whole configuration (eg. duplicati)</li> <li>A reverse proxy for external access (Traefik or nginx)</li> <li>A Network Video Recording (Frigate) to manage cameras ...</li> </ul> <p>For example, leading to a structure like this:</p> <pre><code>graph BT\n  subgraph WIN[Software]\n    direction BT\n    subgraph Docker[WSL2]\n      direction BT\n      d[Docker Deamon] --&gt; c1[Home Assistant]\n      d --&gt; c2[Frigate]\n      d --&gt; c3[...]\n      d --&gt; c4[Zigbee2MQTT]\n      d --&gt; c5[Mosquitto]\n    end\n    USBIP --&gt; Docker\n    Windows --&gt; USBIP\n  end\n  Hardware --&gt; WIN\n  subgraph Hardware[Desktop Hardware]\n    direction LR\n    ZigBee --&gt; USB\n    RFLink --&gt; USB\n  end</code></pre>"},{"location":"docker-from-compose-to-swarm/#thinking-of-the-migration-to-a-cluster-docker-swarm","title":"Thinking of the migration to a cluster... (Docker Swarm)","text":"<p>From here, I still didn't have my Turing Pi and many nodes, but I wanted to get ready for it.  Actually, quick research taught me it's possible to run the same containers in Swarm mode on 1 node. Docker Swarm mode is designed to manage multiple machines (called nodes) each having their own docker instance, and distributing the containers between the machines. So it actually works with only 1 machine, but you don't get the redundancy of multiple node of course. The other good news is the switch to Swarm Mode (in Docker) is reversible: you can switch to Swarm Mode (with one machine) and \"exit\" Swarm Mode to return to the \"normal\" compose file reference - so you don't risk much in preparing files/config for Swarm. And you'll see later that you can leave your Swarm Mode on and still \"compose up\" on the same machine.</p>"},{"location":"docker-from-compose-to-swarm/#looking-at-swarm-mode-what-changes-vs-compose","title":"Looking at Swarm Mode: what changes vs Compose?","text":"<p>A couple of questions came up when thinking of the migration to Swarm Mode:</p> <ul> <li>How to deploy a compose file in swarm mode?</li> <li>How to access bound folders in swarm mode?</li> <li>How to access USB devices in swarm mode?</li> <li>How to restart malfunctioning containers/services? (like self-healing from k8s)</li> </ul> <p>Let's go through these questions one by one</p>"},{"location":"docker-from-compose-to-swarm/#1-how-to-deploy-a-compose-file-in-swarm-mode","title":"1. How to deploy a compose file in swarm mode?","text":"<p>Before switching to the cluster, you can test the updated compose file for Swarm Mode, by deploying the stack on a Swarm of 1 node. And if it doesn't work, remove the stack and compose up instead. No need to exit/delete the swarm to use the regular compose up.</p> <ul> <li>To deploy the compose file on a Swarm:   <pre><code>docker stack deploy --compose-file docker-compose.yaml homeassistant\n</code></pre></li> </ul> <p>...and once finished</p> <ul> <li>To stop it:   <pre><code>docker stack rm homeassistant\n</code></pre></li> </ul>"},{"location":"docker-from-compose-to-swarm/#but-the-compose-file-doesnt-work-as-is-in-swarm-mode-what-needs-to-be-changed-to-work-with-swarm-mode","title":"...but the compose file doesn't work AS-IS in Swarm Mode! What needs to be changed to work with Swarm Mode?","text":"<p>Docker compose reference is slightly different for swarm mode, i.e. some keywords are ignored or not working in swarm mode.</p> <p>This is not a comprehensive list (refer to the compose reference for that), but that's all the things I had to tinker with in my compose file to make it swarm friendly:</p> <ul> <li><code>privileged: true</code> <ul> <li>Ignored in swarm. You should not need it </li> </ul> </li> <li><code>restart: always</code><ul> <li>Ignored in Swarm. Not an issue as swarm services will be destroyed / recreated as necessary (to reach the constraints)</li> </ul> </li> <li><code>container_name:</code> <ul> <li>Ignored in swarm. Doesn't really matter anyway?</li> </ul> </li> <li><code>volume / binds (inline):</code> <ul> <li>Similar to the USB devices, you can't mount bind on a node, while services are moving potentially on any node of the cluster.</li> </ul> </li> <li><code>device:</code> <ul> <li>Ignored in swarm. Ok this one is an issue, particularly with our USB devices needed to communicate with the real world (Zigbee bridge, RFlink or Coral accelerator etc.)</li> </ul> </li> <li><code>shm_size:</code><ul> <li>Ignored in swarm. This is a buffer for cameras. See below for a workaround</li> <li>https://docs.frigate.video/installation/#calculating-required-shm-size</li> </ul> </li> </ul>"},{"location":"docker-from-compose-to-swarm/#workaround-for-shm_size-in-swarm-mode","title":"Workaround for shm_size in Swarm Mode","text":"<p>Since the parameter shm_size does not work in Swarm Mode, you can replace it with a tmpfs volume, as below:</p> <pre><code>services:\n(...)\n  frigate:\n  (...)\n    volume:\n    (...)\n      - type: tmpfs # Optional: 1GB of memory, reduces SSD/SD Card wear\n        target: /tmp/cache\n        tmpfs:\n          size: 1000000000\n    # shm_size: \"190mb\" # update for your cameras based on calculation below (each Tapo is ~27mb and G4 is ~53mb)\n    # (width * height * 1.5 * 9 + 270480)/1048576 = &lt;shm size in mb&gt; for each camera\n    # The below volume replaces shm_size for swarm mode\n      - type: tmpfs\n        target: /dev/shm\n        tmpfs:\n           size: 190000000 # (this means 190mb)\n</code></pre> <p>The next sections focus on the volume / folders access in swarm, then on how to passe a USB device.</p>"},{"location":"docker-from-compose-to-swarm/#2-how-to-access-bound-folders-in-swarm-mode-volumes-and-storage","title":"2. How to access bound folders in swarm mode? (Volumes and storage)","text":"<p>In swarm mode, you can't bind a local folder, because you can't predict on which node a given container will be located. You could force the assignment of a container to a given node with docker constraints and labels, but that would defeat the purpose of running a cluster (ie. having container able to move around based on the availability of the underlying machines (node)).</p> <p>I found two approaches that seem to mitigate this:</p> <ul> <li>Have a filesystem that replicates the content over all nodes (ie. each Raspberry Pi or RK1 modules would have a copy of the entire dataset). Sounds great for data access performance, but obviously it requires enough space on each node and as the lab grows up, will it scale on those poor little 32GB sd cards? Just that idea stopped me from exploring this option further.</li> <li>Or access the data from a network share - which means having all the date in 1 place (one node, or somewhere else...), open it to be shared on the network (eg. NFS) and access it from the nodes. One way to configure this on the container side is to use volumes, see below the description</li> </ul> <p>Volumes in swarm</p> <p>A volume is a way for docker to describe a mount point. When a volume get created, it doesn't actually get physically mounted anywhere until a container needs it. So if you have a docker swarm and multiple nodes, when you create a volume, essentially the description of the volume gets replicated on each nodes but nothing else happens. When a container boot up, it will try to mount a volume on the host its being booted up. If the volume wasn't physically present it will get mounted/created for the first time and reused there. So if you're using the local driver, it will essentially create the folder and that's it. If you have multiple hosts, it means each host will create its own folder on demand.</p> <p>Practically, I would use volumes in the compose file (which was already the case) and replace the \"bound\" volumes with NFS ones</p> <p>For instance, the first volume in the compose file was: </p><pre><code>(...)\nvolumes:\n  hass-config:\n    driver: local\n    driver_opts:\n      type: none\n      device: D:\\HomeAssistant\\hass\\config\n      o: bind\n(...)\n</code></pre> ...and would now look something like: <pre><code>(...)\nvolumes:\n  hass-config:\n    driver: local\n    driver_opts:\n      type: nfs4\n      o: nfsvers=4,addr=&lt;IP-OF-THE-NFS-SERVER&gt;,nolock,rw,sync\n      device: \":/&lt;nfssharepath&gt;/HomeAssistant/hass/config\"\n(...)\n</code></pre><p></p>"},{"location":"docker-from-compose-to-swarm/#3-how-to-access-usb-devices-in-swarm-mode","title":"3. How to access USB devices in swarm mode?","text":"<p>Couple things to deal with:</p> <ol> <li> <p>Problem: by construction, services in a swarm can be run on any node... which will not work if we need to access a specific USB device plugged into it. Can't wait to be lucky and be on the right node...   Solution:</p> <ul> <li>For this we are going to use the swarm specific section \"deploy\" to indicate a constraints (=rule) for the service needing the USB device. It requires \"tagging\" the node with labels in the first place, then using docker deploy constraints. </li> <li>In the example below, the constraint says the node should have a label named \"usb2\" with a value equal to \"true\". <pre><code>services:\n    myService:\n        image: (...)\n        (...)        \n        deploy:\n            placement:\n                constraints: [node.labels.usb2 == true]\n            replicas: 1\n</code></pre></li> <li>Btw, adding such a label to a node is achieved by executing the following command on a manager node:     <pre><code>docker node update --label-add usb2=true &lt;name of the node&gt;\n</code></pre></li> </ul> </li> <li> <p>Even on the right node, how can we pass the USB device to the docker container... without the \"device\" section?</p> <ul> <li>This one is trickier, we have to pass the device as a volume, and manually authorise the device for the container using a number of scripts   This is described in the next section in more details.</li> </ul> </li> <li> <p>A pre-requisite: switch to cgroups v1 (vs v2)</p> <ul> <li>cgroups are used to limit the amount of memory that is available to a particular group of processes (not sure how that definition is relevant in this case). In the below, we use cgroups directly authorisation to allow a container to access a device, using its path, passing it as a volume.</li> <li>Modern linux distro adopted cgroups v2 architecture around 2016... and the trick to access a device and mount it as a volume (described in next section in details) requires cgroups v1. I only realised that when switching to the cluster, as for some reason (maybe an old kernel), WSL was still using cgroups v1 and everything was working fine.</li> <li>To know which version your system is using, run the following:<ul> <li><code>stat -fc %T /sys/fs/cgroup/</code><ul> <li>For cgroup v2, the output is <code>cgroup2fs</code>.</li> <li>For cgroup v1, the output is <code>tmpfs</code>.</li> </ul> </li> </ul> </li> <li>To fore the system to use cgroups v1, on a raspberry, edit the file <code>/boot/cmdline.txt</code> and add:<ul> <li><code>systemd.unified_cgroup_hierarchy=false</code></li> </ul> </li> <li>Not sure what those commands do, but I'll keep them here for now as I got them while researching (maybe for the non raspberry devices, like RK1?)<ul> <li>sudo mkdir /sys/fs/cgroup/devices</li> <li>sudo mount -t cgroup -o devices none /sys/fs/cgroup/devices</li> </ul> </li> </ul> </li> </ol>"},{"location":"docker-from-compose-to-swarm/#create-rules-to-mount-the-device-as-a-volume","title":"Create rules to mount the device as a volume","text":"<p>For the container to be allowed to access the device while passing a volume, we need to give the authorisation (via cgroups) to the container. Since the container ID changes at each execution, we need the scripts and wrappers below to automatically find the right information and create the authorisation.</p> <p>The idea is as follows:</p> <ol> <li>The UDEV rule detects a USB device with a given vendor id and product id, then assigns it a name (symlink) and runs a 'docker-setup-device.sh' shell script.</li> <li>The 'docker-setup-device.sh' script finds the CID of a given container, by name and adds the authorisations for the USB device to the devices.allow file for this container.</li> <li>Device access authorisations are hardcoded for the Coral to <code>c 189:* rwm</code> because it changes vendor and product id after first inference (ie the USB device is different when you plug it and after first access)</li> <li>The service makes sure the script in 2. is executed regularly (because the UDEV rule is only activated when the USB device is plugged)</li> <li>A script is run by the service, to loop-execute the script in 2.</li> </ol> <p>Example with my zigbee USB stick (since then, I upgraded to a ETH version, but that still applies to the RFlink)</p> <ol> <li> <p>UDEV rule: <code>/etc/udev/rules.d/99-zigbee.rules</code></p> 99-zigbee.rules<pre><code>SUBSYSTEM==\"tty\", ATTRS{idVendor}==\"10c4\", ATTRS{idProduct}==\"ea60\", SYMLINK+=\"zigbee\",  RUN+=\"/usr/local/bin/docker-setup-zigbee.sh\"\n</code></pre> </li> <li> <p>Shell script to set up the permissions: <code>/usr/local/bin/docker-setup-zigbee.sh</code></p> docker-setup-zigbee.sh<pre><code>#!/bin/bash\nUSBDEV=`readlink -f /dev/zigbee` # (1)!\nread minor major &lt; &lt;(stat -c '%T %t' $USBDEV)\nif [[ -z $minor || -z $major ]]; then\n  echo 'Device not found'\n  exit\nfi\ndminor=$((0x${minor}))\ndmajor=$((0x${major}))\nCID=`docker ps -a --no-trunc | grep zigbee2mqtt | head -1 |  awk '{print $1}'` # (2)!\nif [[ -z $CID ]]; then\n  echo 'CID not found'\n  exit\nfi\necho 'Setting permissions'\necho \"c $dmajor:$dminor rwm\" &gt; /sys/fs/cgroup/devices/docker/$CID/devices.allow\n</code></pre> <ol> <li>Change the name of the device, <code>zigbee</code> (to be the same as the SYMLINK name in the UDEV rule in previous step)</li> <li>Change the name of the container that needs access to the device, <code>zigbee2mqtt</code> in this example</li> </ol> </li> <li> <p>Service to launch the setup script all the time: <code>/etc/systemd/system/docker-event-listener-zigbee.service</code></p> docker-event-listener-zigbee.service<pre><code>[Unit]\nDescription=Docker Event Listener for USB devices\nAfter=network.target\nStartLimitIntervalSec=0\n[Service]\nType=simple\nRestart=always\nRestartSec=1\nUser=root\nExecStart=/bin/bash /usr/local/bin/docker-event-listener-zigbee.sh\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> </li> <li> <p>Script triggered by the service: <code>/usr/local/bin/docker-event-listener-zigbee.sh</code></p> docker-event-listener-zigbee.sh<pre><code>#!/bin/bash\ndocker events --filter 'event=start'| \\\nwhile read line; do\n  /usr/local/bin/docker-setup-zigbee.sh\ndone\n</code></pre> </li> </ol> <p>Source and more details: https://github.com/Koenkk/zigbee2mqtt/issues/2049</p>"},{"location":"docker-from-compose-to-swarm/#4-cluster-configuration-and-deployment-ansible","title":"4. Cluster configuration and deployment (Ansible)","text":"<p>At that point, I sorted all the questions I had to \"translate\" the compose file to work in Swarm Mode. Thinking about the cluster, you have to install docker and configure multiple machines by definition, personally, I still don't like to do things directly on the machine and if it needs to be changed, to have to figure what I did x years ago... Enter Ansible </p> <p>Ansible enables to describe a configuration state in a yaml file (Ansible playbook), and apply it to a single or set of machines. So instead of changing the target machines directly, you list tasks in the playbook and \"run\" the playbook... and Ansible will connect to the machines via SSH to change the configuration of them according to the playbook.</p>"},{"location":"docker-from-compose-to-swarm/#set-cgroups-config","title":"Set cgroups config","text":"<p>The following ansible playbook snippet sets the configuration of nodes, in particular reverting to cgroup version 1 to enable the use of devices.allow and mounting the device as a volume in docker (as covered in 3. How to access USB devices in swarm mode?).</p> <pre><code>- name: Set up Raspberry cgroup configuration.\n  hosts: cluster\n  tags: init_cgroup\n  gather_facts: true\n  become: true\n\n  handlers:\n    - name: reboot-pi\n      reboot:\n\n  vars_files:\n    - config.yml\n\n  tasks:\n    #cgroups are used to limit the amount of memory that is available to a particular group of processes\n    #systemd.unified_cgroup_hierarchy=false switches back to cgroupv1 (for the USB devices.allow entry to work)\n    - name: Ensure cgroups are configured correctly in cmdline.txt.\n      ansible.builtin.replace:\n        path: /boot/cmdline.txt\n        regexp: '^([\\w](?!.*\\b{{ item }}\\b).*)$'\n        replace: '\\1 {{ item }}'\n      with_items:\n        - \"cgroup_memory=1\"\n        - \"cgroup_enable=memory\"\n        - \"systemd.unified_cgroup_hierarchy=false\"\n      notify: reboot-pi\n      when: ansible_facts['env']['SUDO_USER'] == \"pi\" # only applies on Raspberry Pis\n</code></pre>"},{"location":"docker-from-compose-to-swarm/#rest-of-the-playbook","title":"Rest of the playbook","text":"<p>My playbook is based on Jeff Geerling's Turing Pi 2, where I swapped K3s for Docker Swarm Mode: https://github.com/geerlingguy/turing-pi-2-cluster</p> <p>The rest of the playbook is doing the following steps:</p> <ul> <li>Install docker on all nodes</li> <li>Configure the swarm manager (swarm join, save the token...)</li> <li>Add the worker nodes to the swarm (thanks to the manager token saved)</li> <li>Add labels to nodes</li> <li>Configure USB devices per the above \"trick\"</li> <li>Set a portainer service on boot</li> <li>Configure macvlan network</li> <li>Install keepalived and node_exporter</li> </ul> <p>Create an issue on GitHub if you have a question on any part of it.</p>"},{"location":"docker-from-compose-to-swarm/#random-qa-about-the-cluster","title":"Random Q&amp;A about the cluster","text":""},{"location":"docker-from-compose-to-swarm/#how-to-give-a-real-ip-on-the-lan-while-in-docker-swarm","title":"How to give a \"real\" IP on the LAN while in Docker Swarm?","text":"<p>Initial problem: I switch Traefik as a reverse proxy (from nginx), reinstalled/reconfigured crowdsec... only to realise in Traefik <code>access.log</code> that all \"external IP\" (check it's actually the name in traefik) where all 10.0.0.2, even when accessing my instance from Internet (from my router forwarding to Traefik). This is the IP of a Docker router part of the <code>ingress</code> network (one of the default network created in swarm). Basically, traefik can see IPs from docker network but not the actual public IPs from clients... However, that's the IP field used by crowdsec to decide if it should ban or let through the proxy. So I needed to give Traefik access to the real IP, and that's where the macvlan driver from Docker network comes into play.</p> <p>The macvlan driver allows the container to display a MAC address to your home network (eg. your 192.168.0.0/24) and virtually be part of it.</p> <p>The catch: by construction, a macvlan network needs to know the actual network interface of the machine to talk to it - which may vary in a cluster (swarm). So you can't create a macvlan network in one go on Docker Swarm.</p> <p>The solution: </p> <ul> <li>create a configuration for a macvlan network, on each node of the cluster</li> <li>then create the actual network from a manager of the swarm</li> </ul> <p>In my case, it looks like (extract from my main ansible playbook): </p><pre><code>  - name: Create macvlan network config\n    command: docker network create --config-only --subnet {{ docker_macvlan_subnet }} --gateway {{ docker_macvlan_gateway }} --ip-range {{ docker_macvlan_ip_range }} -o parent=eth0 {{ docker_macvlan_config_name }}\n</code></pre> Where:<p></p> <ul> <li><code>docker_macvlan_subnet</code> is your physical subnet to join, eg. 192.168.0.0/24</li> <li><code>docker_macvlan_gateway</code> is your gateway in that network, eg. 192.168.0.1. This is important for the next parameter to work as intended.</li> <li><code>docker_macvlan_ip_range</code> is the range of IP for the containers joining that network. Here I want a static IP for Traefik, so I assigned a \"range of 1 IP\", for instance 192.168.0.101/32. The /32 means 32 bits of the IP are used, hence this \"range\" is actually the 1 IP provided. This variable has been set to a different value for each node, to avoid IP clashes. </li> <li><code>parent=eth0</code> is the name of the network adapter (physical one) from the node.</li> <li><code>docker_macvlan_config_name</code> is the name of the docker network config file that will be used to create the network at the swarm level (ie. need to have one with the same name on each node to be valid).</li> </ul> <p>And to create the macvlan network: </p><pre><code>  - name: Create macvlan network\n    command: docker network create --config-from {{ docker_macvlan_config_name }} -d macvlan --scope swarm --attachable {{ docker_macvlan_net_name }}\n</code></pre> Where:<p></p> <ul> <li><code>docker_macvlan_config_name</code> is the name of the docker network config file above.</li> <li><code>docker_macvlan_net_name</code> is the name of the actual macvlan network your service will be joining.</li> <li>Note the <code>scope swarm</code>, and <code>attachable</code> parameters, as swarm mode services can only join networks scoped at the swarm level (ie. not a local network), so it also needs to be attachable.</li> </ul> <p>After that and to wrap up, I had to update my local DNS entries to point to the newly acquired IP for Traefik (ie. the 192.168.0.101 above, instead of the usual IP of my cluster).</p> <p>And TADA, my access.log change from having only 10.0.0.2 IPs to displaying beautiful external IPs from the clients, like 148.252.141.15: </p><pre><code>10.0.0.2 - - [23/Sep/2023:13:49:35 +0100] \"GET /apis/tasks HTTP/2.0\" 304 0 \"-\" \"-\" 605 \"websecure-swarmviz@docker\" \"IP-REDACTED\" 65ms\n148.252.141.15 - - [23/Sep/2023:13:49:43 +0100] \"GET /api/history/period/2023-09-23T12:49:29.348Z?filter_entity_id=sensor.processor_use&amp;end_time=2023-09-23T12:49:44.087Z&amp;skip_initial_state&amp;minimal_response HTTP/2.0\" 200 10 \"-\" \"-\" 607 \"websecure-homeassistant-router@file\" \"IP-REDACTED\" 36ms\n</code></pre><p></p>"},{"location":"docker-from-compose-to-swarm/#how-to-restart-all-services-if-the-cluster-cycle-power","title":"How to restart all services if the cluster cycle power?","text":"<p>Because container (stacks) are launched and managed by portainer, I wanted portainer to restart automatically if the machine reboots, so it restarts the other containers subsequently. I decided to create a single compose file for the service \"portainer\" (and, in passing, the custom overlay network for others services to join).</p> <p>From portainer, the rest of the containers (in stacks) will be launched. A template is created to recreate/launch easily. Doing it from portainer allows to control the stack from the web rather than command line on the machine.</p>"},{"location":"docker-from-compose-to-swarm/#access-denied-on-files-from-a-service-eg-node-red-files","title":"Access denied on files from a service? eg. node-red files","text":"<p>After setting up a new service, I often get \"access denied\" errors on volumes accessed by that service in the cluster. The example of Node-Red below is a good one to illustrate how to solve it. A change in Node-Red version is described below:</p> <p>Note: Users migrating from version 0.20 to 1.0 will need to ensure that any existing /data directory has the correct ownership. As of 1.0 this needs to be 1000:1000. This can be forced by the command <code>sudo chown -R 1000:1000 path/to/your/node-red/data</code> From 'Running under Docker' (node-red doc)</p> <p>In my case, on my storage node, I ran: </p><pre><code>sudo chown -R 1000:1000 /zfsdata/nfsshare/HomeAssistant/node-red\n</code></pre><p></p>"},{"location":"docker-from-compose-to-swarm/#all-esphome-devices-appear-offline","title":"All ESPHome devices appear offline?","text":"<p>On the ESPHome home page, my devices show \"offline\". This is due to ESPHome not resolving the device-name.local names to IP address (via mDNS), probably because of the routing in the cluster/docker networks.</p> <p>I've used a workaround for some time, by adding the following parameters to the ESPHome Docker service in docker-compose.yaml, which hardcodes the IP addresses of the devices, in the hosts file of the container: </p><pre><code>    extra_hosts:\n      - \"ep1.local:&lt;IP of the device&gt;\"\n      - \"home-assistant-glow.local:&lt;IP of the device&gt;\"\n</code></pre><p></p> <p>My \"fix\" has been to repack the ESPHome image and adding avahi-utils to it, and allowing the container to access the dbus and avahi socket of the host (because the host resolved the *.local properly).</p> <p>To do that, I'm using the following dockerfile (image published in docker hub poulti/esphome-avahi): </p>Dockerfile<pre><code>FROM esphome/esphome:latest\n\n# Install Avahi for mDNS (resolving .local names)\nRUN apt-get update &amp;&amp; apt-get install avahi-utils -y\n\n# Remove apt cache (from https://docs.docker.com/develop/develop-images/dockerfile_best-practices/)\nRUN rm -rf /var/lib/apt/lists/*\n</code></pre><p></p> <p>And I added the following volumes to the ESPHome Docker service in docker-compose.yaml: </p><pre><code>    volumes:\n      (...)\n      # Added Avahi deamon socket to run avahi-browse using the cache from the host node\n      - /var/run/dbus:/var/run/dbus\n      - /var/run/avahi-daemon/socket:/var/run/avahi-daemon/socket\n</code></pre><p></p>"},{"location":"docker-from-compose-to-swarm/#migration-steps-from-desktop-to-turing-pi-cluster","title":"Migration steps from desktop to Turing Pi cluster","text":"<p>Switching from Docker running on a Windows desktop to a Turing pi Cluster, here are all the steps I took to migrate</p> <p>Physical world: - [ ] Flash modules: 2x raspberry emmc, 1x micro sd card raspberry, 1x RK1 emmc - [ ] Install mobo and power supply, connect Pi, RK1, SSDs - [ ] Test boot OK and connect to Turing Pi BMC (https://turingpi/ by default) - [ ] Configure USB2 to the right Turing Pi node On my computer: - [ ] Run first Ansible ping / accept SSH keys - [ ] Check Ansible config.yml for correct true/false settings (eg. to run the install of storage) - [ ] Test Ansible setup playbooks (swarm activation + NFS setup) by adding <code>--check</code> to the command - [ ] Copy the container config data from desktop disk to the NFS storage Docker compose yaml: - [ ] Change Docker Volumes to NFS binds - [ ] Change Frigate image to the right CPU arch type Physical world: - [ ] Connect USB devices to the right USB ports on Turing Pi - [ ] Change the port forwarding rule from external IP - [ ] Shutdown everything on desktop and deploy the stacks on the cluster</p>"}]}